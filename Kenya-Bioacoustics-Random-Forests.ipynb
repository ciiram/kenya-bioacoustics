{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook reproduces experiments reported in the paper \"Leveraging Citizen Science and Low Cost Recorders for Acoustic Monitoring of Bird Species: A Case Study in Kenya\" by Ciira wa Maina. The aim of the work is to investigate the use of acoustic models trained using data obtained from citizen scientists in automatic bird species recognition. We use data from the citizen science website [Xeno Canto](http://www.xeno-canto.org/) to train models for bird species recognition and test these models on data collected at the [Dedan Kimathi University of Technology](https://www.dkut.ac.ke/) [wildlife conservancy](https://conservancy.dkut.ac.ke/).\n",
    "\n",
    "These audio data were collected using a low cost Raspberry Pi based recorder and are available on [Data Dryad]( http://dx.doi.org/10.5061/dryad.69g60). The recordings are described in the paper [\"A Bioacoustic Record of a Conservancy in the Mount Kenya Ecosystem.\"](https://bdj.pensoft.net/articles.php?id=9906)\n",
    "\n",
    "## Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing\n",
    "Let's get information on bird species present in the audio recordings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#all necessary imports\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "import scipy as sp\n",
    "from scipy import signal\n",
    "import scipy.io.wavfile\n",
    "import librosa\n",
    "import librosa.display\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import pairwise_distances,accuracy_score,precision_score,recall_score, f1_score,hamming_loss,classification_report\n",
    "from sklearn.model_selection import KFold,train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import roc_curve,auc\n",
    "from tabulate import tabulate\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn import svm\n",
    "import oskmeans as skm\n",
    "import csv\n",
    "import random\n",
    "import time\n",
    "\n",
    "np.random.seed(123)#for reproducibility\n",
    "t1=time.time()\n",
    "\n",
    "#Download information on all species observed at DeKUWC (both point counts and acoustically). These has information on codes\n",
    "allspecies_table_url='http://bdj.pensoft.net//lib/ajax_srv/article_elements_srv.php?action=download_table_csv&instance_id=3276052&article_id=9906'\n",
    "allspecies_file_name='all_species.csv'\n",
    "urllib.request.urlretrieve(allspecies_table_url,allspecies_file_name)\n",
    "\n",
    "#Download list of species in foreground and background of recordings ( data in the paper)\n",
    "foreground_table_url='http://bdj.pensoft.net//lib/ajax_srv/article_elements_srv.php?action=download_table_csv&instance_id=3276675&article_id=9906'\n",
    "foreground_file_name='foreground.csv'\n",
    "background_table_url='http://bdj.pensoft.net//lib/ajax_srv/article_elements_srv.php?action=download_table_csv&instance_id=3276676&article_id=9906'\n",
    "background_file_name='background.csv'\n",
    "urllib.request.urlretrieve(foreground_table_url,foreground_file_name);\n",
    "urllib.request.urlretrieve(background_table_url,background_file_name);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the list of foreground and background recordings, we will get a list of the 36 species identified in the annotated recordings. We also change the entry for the Common Bulbul (*Pycnonotus barbatus*) to Dark-capped Bulbul (*Pycnonotus tricolor*) to correspond to the IOC taxonomy used in Xeno-canto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_allspecies=pd.read_csv('all_species.csv',sep=';')\n",
    "df_allspecies\n",
    "df_foreground=pd.read_csv('foreground.csv',sep=';')\n",
    "df_background=pd.read_csv('background.csv',sep=';')\n",
    "species= set(df_foreground['Scientific Name'])#.union(set(df_background['Scientific Name']))\n",
    "species=list(species)\n",
    "species.sort()\n",
    "print(len(species))\n",
    "species[species.index('Pycnonotus barbatus')]='Pycnonotus tricolor'\n",
    "df_allspecies.loc[df_allspecies['Scientific Name']=='Pycnonotus barbatus','Four-Letter Code']\n",
    "df_allspecies.loc[df_allspecies['Scientific Name']=='Pycnonotus barbatus','Four-Letter Code']='DCBU'\n",
    "df_allspecies.loc[df_allspecies['Scientific Name']=='Pycnonotus barbatus','Common Name']='Dark-capped Bulbul'\n",
    "df_allspecies.loc[df_allspecies['Scientific Name']=='Pycnonotus barbatus','Scientific Name']='Pycnonotus tricolor'\n",
    "all_species_sp=list(df_allspecies['Scientific Name'])\n",
    "all_species_cn=list(df_allspecies['Common Name'])\n",
    "all_species_code=list(df_allspecies['Four-Letter Code'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of the species, we will examine xeno-canto and determine if there are more than 10 recordings deposited on the website which are jugded to be of quality C or better on an A-E scale. If this is the case, we will download the data and use it to train models. Each time this is run it may download recent uploads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir='/home/ciira/Documents/Research/BiodiversityMonitoring/repo/acoustic-species-detection/data/mp3/'\n",
    "#get recordings already present\n",
    "XC_file=os.listdir(data_dir)\n",
    "min_rec=10\n",
    "\n",
    "#download\n",
    "recording_data=[]\n",
    "number_recordings=[]\n",
    "for sp in species:\n",
    "    webaddress='http://www.xeno-canto.org/api/2/recordings?query='+sp+' cnt:kenya'\n",
    "    response=requests.get(webaddress)\n",
    "    num_rec=0\n",
    "    if response.status_code==200:\n",
    "        data=response.json()\n",
    "        rec=data['recordings']\n",
    "        print ('Successful Query',all_species_cn[all_species_sp.index(sp)],int(data['numRecordings']))\n",
    "        if len(rec)>0:\n",
    "            for j in range(len(rec)):\n",
    "                if rec[j]['rec']!='Ciira Maina' and rec[j]['q']!='no score':#Ignore recordings by author to avoid possible 'known individuals' and recordings with no score\n",
    "                    #print(sp,rec[j]['id'],rec[j]['rec'],rec[j]['url'])\n",
    "                    num_rec+=1\n",
    "        number_recordings.append(num_rec)                    \n",
    "        if num_rec>=min_rec:\n",
    "            for j in range(len(rec)):\n",
    "                if rec[j]['rec']!='Ciira Maina' and rec[j]['q']!='no score':#Ignore recordings by author to avoid possible 'known individuals' and recordings with no score\n",
    "                    #print(sp,rec[j]['id'],rec[j]['rec'],rec[j]['url'])\n",
    "                    if XC_file.count(str(rec[j]['id'])+'.mp3')==0:#recording is not present\n",
    "                        print ('Downloading file ',rec[j]['id'],'for',sp)\n",
    "                        filename=data_dir+rec[j]['id']+'.mp3'\n",
    "                        urllib.request.urlretrieve(rec[j]['file'],filename)\n",
    "                    recording_data.append([rec[j]['id'],rec[j]['en'],sp,rec[j]['rec'],rec[j]['url']])\n",
    "\n",
    "df_xc=pd.DataFrame.from_records(recording_data,columns=['Xeno-canto ID','Common Name','Scientific Name','Recordist','url'])\n",
    "recog_species=list(set(df_xc['Scientific Name']))\n",
    "recog_species.sort()\n",
    "recog_species_cn=[all_species_cn[all_species_sp.index(recog_species[i])] for i in range(len(recog_species))]\n",
    "recog_species_code=[all_species_code[all_species_sp.index(recog_species[i])] for i in range(len(recog_species))]\n",
    "recog_species_numrec=[number_recordings[species.index(recog_species[i])] for i in range(len(recog_species))]\n",
    "\n",
    "#save\n",
    "rows = zip(recog_species,recog_species_cn,recog_species_code,recog_species_numrec)\n",
    "with open(\"recog_species.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    for row in rows:\n",
    "        writer.writerow(row)\n",
    "\n",
    "\n",
    "df_xc.to_csv('recordings.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We convert the mp3 files to wav."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mp3_dir = 'data/mp3/'\n",
    "wav_dir = 'data/wav-16kHz/'\n",
    "mp3s=os.listdir(mp3_dir)\n",
    "wavs=os.listdir('data/wav-16kHz/')\n",
    "\n",
    "for filename in mp3s:\n",
    "    if filename.split('.')[0]+'.wav' not in wavs:\n",
    "        print('Converting...',filename)\n",
    "        cmd ='sox '+mp3_dir+filename+' -r 16000 -c 1 '+wav_dir+filename.split('.')[0]+'.wav'\n",
    "        os.system(cmd)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary visualisations\n",
    "Now we can examine some of the audio data and plot some spectrograms. Figure 3a in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#audio parameters\n",
    "Nfft=512\n",
    "Noverlap=256\n",
    "f_min=300\n",
    "f_max=8000\n",
    "num_mel=80\n",
    "summary='mean_std'\n",
    "learned_features=False\n",
    "patch_width=8\n",
    "patch_height=num_mel\n",
    "#indx=np.random.randint(0,len(df_xc)-1)df_xc['Common Name']\n",
    "indx=244\n",
    "print('The recording is XC%d from Xeno-canto.'%int(df_xc['Xeno-canto ID'][indx]))\n",
    "\n",
    "x = scipy.io.wavfile.read(wav_dir+df_xc['Xeno-canto ID'][indx]+'.wav')\n",
    "fs = float(x[0])\n",
    "x2 = x[1] / 2.**15\n",
    "x2 = x2 / np.max(np.abs(x2))\n",
    "Spec = librosa.stft(x2,n_fft=Nfft, hop_length=Noverlap)\n",
    "librosa.display.specshow(librosa.amplitude_to_db(Spec, ref=np.max),  cmap='gray_r',y_axis='linear', x_axis='time',sr=fs,hop_length=Noverlap);\n",
    "plt.savefig('../../figures/XC_HT_spec.jpg',dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The corresponding melspectrogram. Figure 3b in the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import gridspec\n",
    "S = librosa.feature.melspectrogram(S=np.abs(Spec)**2, sr=fs, n_mels=num_mel,fmin=f_min,fmax=f_max)\n",
    "librosa.display.specshow(librosa.power_to_db(S, ref=np.max), cmap='gray_r',y_axis='mel', x_axis='time',sr=fs,hop_length=Noverlap,fmin=f_min,fmax=f_max)\n",
    "\n",
    "gs = gridspec.GridSpec(1, 2, width_ratios=[3, 1]) \n",
    "ax0 = plt.subplot(gs[0])\n",
    "librosa.display.specshow(librosa.power_to_db(S, ref=np.max), cmap='gray_r',y_axis='mel', x_axis='time',sr=fs,hop_length=Noverlap,fmin=f_min,fmax=f_max)\n",
    "ax1 = plt.subplot(gs[1])\n",
    "ax1.plot(np.mean(S,1),np.arange(num_mel))\n",
    "ax1.set_yticklabels('');\n",
    "plt.savefig('../../figures/XC_HT_mel.jpg',dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now obtain features from the data for use in training and testing our classifiers. We will use the following features:\n",
    "1. The melspectrogram summarised using the mean and standard deviation\n",
    "2. Learned features from time-frequency patches of the melspectrogram\n",
    "\n",
    "## Melspectrogram Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_melspec(filename,Nfft,Noverlap,f_min,f_max,num_mel,summary):\n",
    "    x = scipy.io.wavfile.read(filename)\n",
    "    x2 = x[1] / 2.**15\n",
    "    x2 = x2 / np.max(np.abs(x2))\n",
    "    fs = float(x[0])\n",
    "    Spec = librosa.stft(x2,n_fft=Nfft, hop_length=Noverlap)\n",
    "    Spec=np.abs(Spec)\n",
    "    S = librosa.feature.melspectrogram(S=np.abs(Spec)**2, sr=fs, n_mels=num_mel,fmin=f_min,fmax=f_max)\n",
    "    S/=S.max()\n",
    "    meanS=np.mean(S,1)\n",
    "    \n",
    "    if summary=='mean':\n",
    "        return meanS\n",
    "    elif summary=='mean_std':\n",
    "        return np.concatenate((meanS,np.std(S,1)))\n",
    "    elif summary=='max':\n",
    "        return np.max(S,1)\n",
    "        \n",
    "\n",
    "\n",
    "def melspec_time_frequency_patch(filename,Nfft,Noverlap,f_min,f_max,num_mel,patch_height,patch_width):\n",
    "    x = scipy.io.wavfile.read(filename)\n",
    "    x2 = x[1] / 2.**15\n",
    "    x2 = x2 / np.max(np.abs(x2))\n",
    "    fs = float(x[0])\n",
    "    Spec = librosa.stft(x2,n_fft=Nfft, hop_length=Noverlap)\n",
    "    S = librosa.feature.melspectrogram(S=np.abs(Spec)**2, sr=fs, n_mels=num_mel,fmin=f_min,fmax=f_max)\n",
    "    S/=S.max()\n",
    "    feature=[]\n",
    "\n",
    "    for i in range(0,S.shape[0]-patch_height+1,patch_height):\n",
    "        for j in range(0,S.shape[1]-patch_width+1,patch_width):\n",
    "            TF_patch=S[i:i+patch_height,j:j+patch_width]\n",
    "            feature.append(np.ndarray.flatten(TF_patch))\n",
    "        \n",
    "    return np.array(feature)\n",
    "\n",
    "\n",
    "features=[]\n",
    "labels=[]\n",
    "\n",
    "\n",
    "for i in range(len(df_xc)):\n",
    "    print('Processing XC%d'%int(df_xc['Xeno-canto ID'][i]))\n",
    "    features.append(avg_melspec(wav_dir+df_xc['Xeno-canto ID'][i]+'.wav',Nfft,Noverlap,f_min,f_max,num_mel,summary))\n",
    "    labels.append(recog_species.index(df_xc['Scientific Name'][i]))\n",
    "\n",
    "\n",
    "X=np.array(features)\n",
    "labels=np.array(labels)\n",
    "y=label_binarize(labels,np.unique(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now test the performance of a random forest classifier by determining the precision, recall and F1 score for each species using 20 random splits of the data into 70% for training and 30% for testing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NT=500 #number of trees\n",
    "num_trial=20\n",
    "test_proportion=.3\n",
    "species_precision=np.zeros((len(recog_species),num_trial))\n",
    "species_recall=np.zeros((len(recog_species),num_trial))\n",
    "species_f1=np.zeros((len(recog_species),num_trial))\n",
    "trial_pred_scores=[]\n",
    "trial_y_test=[]\n",
    "for trial in range(num_trial):\n",
    "    X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=test_proportion)\n",
    "    print('Training random forest classifier...')\n",
    "    clf= OneVsRestClassifier(RandomForestClassifier(n_estimators=NT,criterion='entropy'))\n",
    "    clf.fit(X_train,y_train)\n",
    "    result=clf.predict(X_test)\n",
    "    scores=clf.predict_proba(X_test)\n",
    "    top_res=np.zeros(result.shape)\n",
    "    top_3_res=np.zeros(result.shape)\n",
    "    trial_pred_scores.append(scores)\n",
    "    trial_y_test.append(y_test)\n",
    "\n",
    "    print('Screening...')\n",
    "    for i in range(result.shape[0]):\n",
    "        line=''\n",
    "        for j in range(len(y_test[i,:].nonzero()[0])):\n",
    "            line+=recog_species_code[y_test[i,:].nonzero()[0][j]]+' '\n",
    "        line+=';'\n",
    "\n",
    "        indx=np.argsort(scores[i,:])[::-1][0:3].tolist()\n",
    "        top_res[i,indx[0]]=1\n",
    "        top_3_res[i,indx]=1\n",
    "        file_species=[recog_species_code[i] for i in indx]\n",
    "        for j in range(len(file_species)):\n",
    "            line+=file_species[j]+' '\n",
    "\n",
    "        print(line)\n",
    "    for i in range(len(recog_species)):\n",
    "        species_precision[i,trial]=precision_score(y_test[:,i],top_res[:,i])\n",
    "        species_recall[i,trial]=recall_score(y_test[:,i],top_res[:,i])\n",
    "        species_f1[i,trial]=f1_score(y_test[:,i],top_res[:,i])\n",
    "         \n",
    "print(classification_report(y_test, top_res, target_names=recog_species_cn))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Produce box plots of the precision, recall and F1 score. Figure 4 in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.boxplot(species_precision.T)\n",
    "plt.xticks(range(1,len(recog_species)+1), recog_species_code, rotation='vertical')\n",
    "plt.ylabel('Precision')\n",
    "plt.savefig('../../figures/xc_precision_mel.jpg',dpi=300)\n",
    "\n",
    "plt.figure()\n",
    "plt.boxplot(species_recall.T)\n",
    "plt.xticks(range(1,len(recog_species)+1), recog_species_code, rotation='vertical')\n",
    "plt.ylabel('Recall')\n",
    "plt.savefig('../../figures/xc_recall_mel.jpg',dpi=300)\n",
    "\n",
    "plt.figure()\n",
    "plt.boxplot(species_f1.T)\n",
    "plt.xticks(range(1,len(recog_species)+1), recog_species_code, rotation='vertical')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.savefig('../../figures/xc_f1_mel.jpg',dpi=300)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a table with the median precision, recall and F1 score. Table 2 in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_species_metrics=pd.DataFrame(list(zip(recog_species_code,np.median(species_precision,1).tolist(),np.median(species_recall,1).tolist(), np.median(species_f1,1).tolist())),columns=['Species','Precision','Recall','F1 Score'])\n",
    "df_species_metrics=df_species_metrics.sort_values(['Precision'], ascending=False)\n",
    "df_species_metrics=df_species_metrics.reset_index(drop=True)\n",
    "print(df_species_metrics.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Screen DeKUWC recordings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train with all the xeno-canto data\n",
    "clf_mel= OneVsRestClassifier(RandomForestClassifier(n_estimators=NT,criterion='entropy'))\n",
    "clf_mel.fit(X,y)\n",
    "\n",
    "df_dekuwc=pd.read_csv('data/DeKUWC/PiRecordingsAnnotation.csv')\n",
    "dekut_wavs=os.listdir('data/DeKUWC/wav')\n",
    "dekut_wavs.sort()\n",
    "topN=3\n",
    "dekuwc_true_label=np.zeros((len(dekut_wavs),len(recog_species_code)))\n",
    "\n",
    "dekuwc_pred_scores=[]\n",
    "annotation=[]\n",
    "recog_result=[]\n",
    "for filename in dekut_wavs:\n",
    "    print('Processing',filename)\n",
    "    df=df_dekuwc.loc[df_dekuwc['Filename']==filename]\n",
    "    \n",
    "    features=avg_melspec('data/DeKUWC/wav/'+filename,Nfft,Noverlap,f_min,f_max,num_mel,summary)\n",
    "    \n",
    "    file_result=clf_mel.predict_proba(np.array(features).reshape(1, -1))\n",
    "    dekuwc_pred_scores.append(file_result[0,:])\n",
    "    indx=np.argsort(file_result)[0][::-1][0:topN].tolist()\n",
    "    recog_file_species=[recog_species_code[i] for i in indx]\n",
    "    true_file_species=df.iloc[0,1].split(';')\n",
    "    if 'COBU' in true_file_species:\n",
    "        true_file_species[true_file_species.index('COBU')]='DCBU'\n",
    "    for i in range(len(true_file_species)):\n",
    "        if true_file_species[i] in recog_species_code:\n",
    "            dekuwc_true_label[dekut_wavs.index(filename),recog_species_code.index(true_file_species[i])]=1\n",
    "    \n",
    "    dd=recog_file_species[0]\n",
    "    for i in range(1,len(recog_file_species)):\n",
    "        dd+=';'\n",
    "        dd+=recog_file_species[i]\n",
    "    annotation.append(df.iloc[0,1])\n",
    "    recog_result.append(dd)\n",
    "    #print(true_file_species+[';']+recog_file_species)\n",
    "    \n",
    "np.array(dekuwc_pred_scores)   \n",
    "df_screening = pd.DataFrame(list(zip(dekut_wavs, annotation, recog_result)),columns=['Filenames','True Species','Recognition Result'])\n",
    "df_screening.to_csv('screening_result_mel.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the screening result for a selection of five files. Table 4 of the paper (melspecrogram feature result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_screening.loc[df_screening['Filenames'].isin(['1-2016-01-05-10-40-01.wav','1-2016-01-05-11-10-01.wav','2-2016-01-06-07-45-01.wav','4-2016-01-06-11-45-01.wav','6-2016-01-29-12-35-01.wav'])].reset_index(drop=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the classification report. Table 3 of the paper. This is obtained by selecting a threshold that optimizes the F1 score for each species and reports that species as present when its probability exceeds this threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species_threshold=np.zeros(len(recog_species_code))\n",
    "species_max_f1=np.zeros(len(recog_species_code))\n",
    "species_max_precision=np.zeros(len(recog_species_code))\n",
    "species_max_recall=np.zeros(len(recog_species_code))\n",
    "thresh=np.linspace(0,1,100)\n",
    "dekuwc_pred_scores=np.vstack(dekuwc_pred_scores)\n",
    "for sp_code in recog_species_code:\n",
    "    f1_scores=np.zeros(len(thresh))\n",
    "    for i in range(len(thresh)):\n",
    "        f1_scores[i]=f1_score(dekuwc_true_label[:,recog_species_code.index(sp_code)],dekuwc_pred_scores[:,recog_species_code.index(sp_code)]>thresh[i]);\n",
    "    \n",
    "    species_max_f1[recog_species_code.index(sp_code)]=np.max(f1_scores)\n",
    "    species_threshold[recog_species_code.index(sp_code)]=thresh[np.argmax(f1_scores)]\n",
    "    species_max_precision[recog_species_code.index(sp_code)]=precision_score(dekuwc_true_label[:,recog_species_code.index(sp_code)],dekuwc_pred_scores[:,recog_species_code.index(sp_code)]>thresh[np.argmax(f1_scores)])\n",
    "    species_max_recall[recog_species_code.index(sp_code)]=recall_score(dekuwc_true_label[:,recog_species_code.index(sp_code)],dekuwc_pred_scores[:,recog_species_code.index(sp_code)]>thresh[np.argmax(f1_scores)])\n",
    "\n",
    "df_dekuwc_metrics=pd.DataFrame(list(zip(recog_species_code,species_max_precision.tolist(),species_max_recall.tolist(), species_max_f1.tolist())),columns=['Species','Precision','Recall','F1 Score'])\n",
    "df_dekuwc_metrics=df_dekuwc_metrics.sort_values(['F1 Score'], ascending=False)\n",
    "df_dekuwc_metrics=df_dekuwc_metrics.reset_index(drop=True)\n",
    "print(df_dekuwc_metrics.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Time taken for melspectrogram features is %.2f minutes'%((time.time()-t1)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hartlaub's Turaco Screening\n",
    "\n",
    "We now screen all the 2701 recordings obtained from the DeKUWC for presence of the Harltaub's Turaco. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1=time.time()\n",
    "df_all_dekuwc=pd.read_csv('data/DeKUWC/AllLocations_Annotation.csv')\n",
    "all_filenames=list(df_all_dekuwc['Filename'])\n",
    "all_dekuwc_pred_score=[]\n",
    "for filename in all_filenames:\n",
    "    print('Processing',filename)\n",
    "    features=avg_melspec('../../data/DeKUWC_PiRecordings2016/wav/'+filename+'.wav',Nfft,Noverlap,f_min,f_max,num_mel,summary)\n",
    "    file_result=clf_mel.predict_proba(np.array(features).reshape(1, -1))\n",
    "    all_dekuwc_pred_score.append(file_result[0,:])\n",
    "    \n",
    "print('Time taken to screen all files is %.2f minutes'%((time.time()-t1)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain the Hartlaub's Turaco recordings and plot spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dekuwc_pred_score=np.vstack(all_dekuwc_pred_score)\n",
    "all_dekuwc_pred_label=np.array(all_dekuwc_pred_score>species_threshold).astype(int)\n",
    "hatu_file=[]\n",
    "plot_fig=True\n",
    "for filename in all_filenames:\n",
    "    if all_dekuwc_pred_label[all_filenames.index(filename),recog_species_code.index('HATU')]:\n",
    "        print(filename)\n",
    "        hatu_file.append(filename)\n",
    "        if plot_fig:\n",
    "            x = scipy.io.wavfile.read('../../data/DeKUWC_PiRecordings2016/wav/'+filename+'.wav')\n",
    "            fs = float(x[0])\n",
    "            x2 = x[1] / 2.**15\n",
    "            x2 = x2 / np.max(np.abs(x2))\n",
    "            Spec = librosa.stft(x2,n_fft=Nfft, hop_length=Noverlap)\n",
    "            librosa.display.specshow(librosa.amplitude_to_db(Spec, ref=np.max),  cmap='gray_r',y_axis='linear', x_axis='time',sr=fs,hop_length=Noverlap);\n",
    "            plt.savefig('HATU-Spectrograms/'+filename+'.jpg',dpi=300)\n",
    "            plt.clf()\n",
    "            \n",
    "df_screening = pd.DataFrame(list(zip(hatu_file)),columns=['Filenames'])\n",
    "df_screening.to_csv('HATU_screening_result_mel.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
